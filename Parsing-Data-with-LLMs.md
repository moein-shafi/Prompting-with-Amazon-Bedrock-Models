# 1- Introduction: Moving to Real-World Data Tasks
Welcome! Up to this point, you have been learning how to use large language models (LLMs) for basic coding tasks. In this unit, we will take things a step further and explore how LLMs can help with more complex, professional tasks — specifically, parsing and analyzing structured data.

You do not need to be a programmer to follow along. This course is designed to be approachable for everyone, including those without a technical background. However, as we start working with more complex data, you may notice that the code generated by the LLM can become more involved, and sometimes, the model might not get things right on the first try. That’s normal! You will learn how to refine your prompts and regenerate responses as needed. We will also cover debugging in future lessons.

Let’s get started by understanding what data parsing is and why it’s useful.

## What Is Data Parsing and Why Use LLMs?
`Data parsing` is the process of taking structured data — like a table, a CSV file, or an XML document — and extracting useful information from it. This is a common task in many jobs, from business analysis to software development.

For example, you might have a CSV file with employee records and want to find the average salary, the most common job title, or the country with the most employees. LLMs can help automate these tasks by generating code or even providing direct answers, saving you time and effort.

Here are some common data formats you might encounter:

- `CSV (Comma-Separated Values)`: Used for spreadsheets and simple databases.
- `XML (eXtensible Markup Language)`: Used for structured documents and data exchange.
- `JSON (JavaScript Object Notation)`: Used for APIs and web data.

LLMs are helpful because they can quickly generate scripts or code to parse these formats, even if you are not a coding expert.

## How to Provide Data Context in Prompts
When asking an LLM to help parse data, you do not need to provide the entire dataset. Instead, you can include a small sample of the data. This gives the model enough context to understand the structure and content while keeping your prompt clear and focused.

It’s also important to format your prompt so the LLM can easily tell which part is the data, which part is your instructions, and what constraints you want to set. Using XML-styled formatting is one of the best options.

Let’s look at how you might present a data sample in your prompt:

```xml
<data_example>
ID, Name, Age, Occupation, Country, Salary, Joining Date, Active
101, Alice Johnson, 30, Engineer, USA, 85000, 2015-06-23, True
102, Bob Smith, 25, Designer, UK, 62000, 2018-09-15, True
103, Charlie Brown, 35, Teacher, Canada, 55000, 2012-01-10, False
</data_example>
```
By including just a few rows, you give the LLM enough information to understand the data’s structure and the types of values it contains.



## Designing Prompts for Data Parsing Tasks: Context
Now, let’s build a step-by-step prompt to help the LLM generate a script for parsing data.

### Step 1: Start with the *Data Sample*

First, include a snippet of your data using clear tags. 

**Important**: Mention the source of the data.

```xml
I have a file called `workers.csv`:

<data_example>
ID, Name, Age, Occupation, Country, Salary, Joining Date, Active
101, Alice Johnson, 30, Engineer, USA, 85000, 2015-06-23, True
102, Bob Smith, 25, Designer, UK, 62000, 2018-09-15, True
</data_example>
```

### Step 2: Designing Prompts for Data Parsing Tasks: *Instructions*
Next, tell the LLM exactly what you want to do with the data. For example:

```xml
<instructions>
Please parse this data and provide the following:

1. Calculate the average salary of all active employees.
2. Identify the country with the highest number of employees.
3. Determine the most common occupation among the employees.
</instructions>
```


### Step 3: Designing Prompts for Data Parsing Tasks: *Constraints*
Consider including the following constraints in your prompt:


- `Provide me with fully runnable code` – ensures the model creates a ready-to-go solution that you don't need to modify yourself.
- `Only create a simple python script, nothing extra` – some models, especially Claude, tend to overcomplicate the requested code. You might get a solution that is difficult to use due to unnecessary extra functions. This constraint ensures the provided solution only addresses your request.
- `Explain the algorithm in simple words` – if you want to learn more about the code model generated. Usually, models do an excellent job explaining their code.

## Full Prompt Example
Here is a complete example of a well-structured prompt for parsing data with an LLM:

```xml
I have a file called `workers.csv`:

<data_example>
ID, Name, Age, Occupation, Country, Salary, Joining Date, Active
101, Alice Johnson, 30, Engineer, USA, 85000, 2015-06-23, True
102, Bob Smith, 25, Designer, UK, 62000, 2018-09-15, True
103, Charlie Brown, 35, Teacher, Canada, 55000, 2012-01-10, False
</data_example>

<instructions>
Please parse this data and provide the following:
1. Calculate the average salary of all active employees.
2. Identify the country with the highest number of employees.
3. Determine the most common occupation among the employees.
</instructions>

<constraints>
- Provide me with fully runnable code.
- Only create a simple python script, nothing extra.
- Explain the algorithm in simple words.
</constraints>
```

This prompt clearly defines the data sample, instructions, and constraints, making it easy for the LLM to understand and respond accurately.




# 2- Introduction: Debugging with LLMs
In the previous lesson, you learned how to create prompts that help LLMs parse and analyze structured data. Now, let’s focus on a different but equally important skill: debugging code with the help of LLMs.

When your code doesn’t work as expected, it can be frustrating. LLMs can be a powerful tool to help you find and fix bugs, but only if you communicate the problem clearly. In this lesson, I will show you how to share the right information and set clear expectations so the LLM can give you the most helpful answer.

## Recognizing When Code Isn’t Working
Before you can ask for help, you need to know when your code is not working. Usually, you will notice one of two things:

- Your code gives an error message and stops running.
- Your code runs, but the result is not what you expected.

The most common way to spot a problem is through an error message called a traceback. A traceback is a report that shows where the error happened in your code and what kind of error it was.

For example, if you run this code:

```Python
numbers = [10, 20, 30, 40, 50]
total = 0
count = 0
for i in numbers:
    total += i
print(total / count)
```

You might see this error message:

```Plain text
Traceback (most recent call last):
  File "test.py", line 6, in <module>
    print(total / count)
          ~~~~~~^~~~~~~
ZeroDivisionError: division by zero
```

This traceback tells you that the code tried to divide by zero, which is not allowed in Python. The error happened on the line print(total / count).

## What to Share with the LLM
When you ask an LLM for help with debugging, it is important to include both your code and the relevant part of the traceback. If you only share the code, the LLM might not know what went wrong. If you only share the error message, the LLM won’t know what your code looks like.

Let’s take a step-by-step look at a good debugging prompt.

```Plain text
I have a Python script that processes a list of numbers and calculates their average. 
However, the script is throwing an error, and I'm unable to identify the issue. 

Here is the original code:
`script.py`
numbers = [10, 20, 30, 40, 50]
total = 0
count = 0
for i in numbers:
    total += i
print(total / count)

Here is the traceback:

Traceback (most recent call last):
  File "test.py", line 6, in <module>
    print(total / count)
          ~~~~~~^~~~~~~
ZeroDivisionError: division by zero
```

Here, you give the LLM all the information it needs to understand the problem and help you fix it.

## Stating Your Constraints Clearly
LLMs tend to provide more changes than required, especially the Claude models. It is likely to include a lot of excess comments and error-handling code, which may be unnecessary.

To keep your code simple, you might want to add the following constraints:

```Plain text
Provide me with an updated code and a short explanation.

# Constraints

- Don't include any error handling in the code
- Exclude comments in the code
```

By stating your constraints, you help the LLM focus on precisely what you want: a fixed version of your code without extra error handling or comments.

## Sharing the Relevant Part of a Long Traceback
Sometimes, your traceback can be very long, especially if your code uses external libraries or frameworks. In these cases, copying the entire traceback into your prompt is not always helpful. Instead, focus on sharing the most relevant part:

Include the last few lines of the traceback, especially the part that shows the error type and the exact line in your code where the error happened.
If the traceback includes many lines from library code, you can skip those and only include the lines that reference your own files or functions.
For example, if you see a long traceback like this:

```Plain text
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 973, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/home/user/my_script.py", line 12, in run
    result = process_data(data)
  File "/home/user/my_script.py", line 8, in process_data
    return sum(data) / len(data)
ZeroDivisionError: division by zero
```

You can share just the relevant part:

This helps the LLM focus on the part of the traceback that is most useful for debugging your code.


# 3- Introduction: Keeping Code Working Over Time
Imagine you have a piece of code that does a specific job for you — maybe it parses some data, generates a report, or automates a task. Over time, you will likely need to update this code. New requirements might come up, bugs may appear, or you might want to improve how the code works. This is a regular part of software development, and it’s called code maintenance.

Large Language Models (LLMs) can help you maintain and develop your code more efficiently. In this lesson, I will show you three practical strategies for using LLMs to update and improve existing code. Each strategy has its strengths and limitations, and I will walk you through how to use them with clear examples.

# Strategy 1: Full Code Replacement
The first and most direct strategy is to provide the LLM with your entire code and ask it to return a fully updated version. This simple approach works well for small scripts or for making straightforward changes.

Let’s look at an example. Imagine you have a code that parses your data, and you want to update the generated report data. Here is a prompt:

```Plain text
I have the following code that parses data from CSV and converts it into a report:

<code omitted for brevity>

# Instructions:
- Improve this code to make the report better structured as a list of bullet points
- Provide me with FULL updated code
```

**Explanation:**

- You start by pasting your code so the LLM has all the context.
- You clearly state what you want: a better-structured report, specifically as a list of bullet points.
- You ask for the full updated code, not just a snippet.

This method is fast and can work well for small to medium-sized scripts. However, the LLM’s consistency and accuracy can drop as your code gets larger. Remember, LLMs are limited to how much code they can process at once, and they might miss details or introduce new issues in bigger files.

## Strategy 2: Building Context with the LLM
When your code is too large or complex for the LLM to handle all at once, you can use the generated knowledge approach to help the LLM build context before making changes.

### Step 1: Ask the LLM to explain your code.

```Plain text
I have the following code that parses data from CSV and converts it into a report:

<code omitted for brevity>

Explain this code in detail.
```

**Explanation:**

- By asking the LLM to explain your code, you help it “understand” what the code does.
- The LLM will summarize the logic, which can also help you spot any issues or areas for improvement.

### Step 2: Now, ask for the changes you want.

```Plain text
Good. Now:
- Improve this code to make the report better structured, as a list of bullet points
- Provide me with FULL updated code
```

**Explanation:**

- After the LLM has explained the code, it has a better context for making changes.
- This two-step process can lead to more accurate and relevant updates, especially for larger codebases.

However, even with this approach, you can still hit the LLM’s context window limit if your application is very large. In those cases, you may need to break your code into smaller parts or use the following strategy.

## Strategy 3: Guided Code Updates
If your codebase is too large to update all at once, you can ask the LLM to guide you through the step-by-step update process.

Here’s how you can do it:

```Plain text
I have the following code that parses data from CSV and converts it into a report:

<code omitted for brevity>

# Instructions:
- Explain steps I need to take to improve this code to make the report better structured, as a list of bullet points
- Include code snippets for each step
- Explain each step in detail
```

**Explanation:**

- Instead of asking for the full updated code, you ask the LLM to break down the process into steps.
- The LLM will list the changes you need to make, provide code snippets for each step, and explain why each change is required.

- This approach is especially useful for large projects or when you want to understand the reasoning behind each change.

- It also helps you learn and build your own coding skills since you’ll be making the changes yourself with the LLM’s guidance.
