# 1- Introduction: Moving to Real-World Data Tasks
Welcome! Up to this point, you have been learning how to use large language models (LLMs) for basic coding tasks. In this unit, we will take things a step further and explore how LLMs can help with more complex, professional tasks — specifically, parsing and analyzing structured data.

You do not need to be a programmer to follow along. This course is designed to be approachable for everyone, including those without a technical background. However, as we start working with more complex data, you may notice that the code generated by the LLM can become more involved, and sometimes, the model might not get things right on the first try. That’s normal! You will learn how to refine your prompts and regenerate responses as needed. We will also cover debugging in future lessons.

Let’s get started by understanding what data parsing is and why it’s useful.

## What Is Data Parsing and Why Use LLMs?
`Data parsing` is the process of taking structured data — like a table, a CSV file, or an XML document — and extracting useful information from it. This is a common task in many jobs, from business analysis to software development.

For example, you might have a CSV file with employee records and want to find the average salary, the most common job title, or the country with the most employees. LLMs can help automate these tasks by generating code or even providing direct answers, saving you time and effort.

Here are some common data formats you might encounter:

- `CSV (Comma-Separated Values)`: Used for spreadsheets and simple databases.
- `XML (eXtensible Markup Language)`: Used for structured documents and data exchange.
- `JSON (JavaScript Object Notation)`: Used for APIs and web data.

LLMs are helpful because they can quickly generate scripts or code to parse these formats, even if you are not a coding expert.

## How to Provide Data Context in Prompts
When asking an LLM to help parse data, you do not need to provide the entire dataset. Instead, you can include a small sample of the data. This gives the model enough context to understand the structure and content while keeping your prompt clear and focused.

It’s also important to format your prompt so the LLM can easily tell which part is the data, which part is your instructions, and what constraints you want to set. Using XML-styled formatting is one of the best options.

Let’s look at how you might present a data sample in your prompt:

```xml
<data_example>
ID, Name, Age, Occupation, Country, Salary, Joining Date, Active
101, Alice Johnson, 30, Engineer, USA, 85000, 2015-06-23, True
102, Bob Smith, 25, Designer, UK, 62000, 2018-09-15, True
103, Charlie Brown, 35, Teacher, Canada, 55000, 2012-01-10, False
</data_example>
```
By including just a few rows, you give the LLM enough information to understand the data’s structure and the types of values it contains.



## Designing Prompts for Data Parsing Tasks: Context
Now, let’s build a step-by-step prompt to help the LLM generate a script for parsing data.

### Step 1: Start with the *Data Sample*

First, include a snippet of your data using clear tags. 

**Important**: Mention the source of the data.

```xml
I have a file called `workers.csv`:

<data_example>
ID, Name, Age, Occupation, Country, Salary, Joining Date, Active
101, Alice Johnson, 30, Engineer, USA, 85000, 2015-06-23, True
102, Bob Smith, 25, Designer, UK, 62000, 2018-09-15, True
</data_example>
```

### Step 2: Designing Prompts for Data Parsing Tasks: *Instructions*
Next, tell the LLM exactly what you want to do with the data. For example:

```xml
<instructions>
Please parse this data and provide the following:

1. Calculate the average salary of all active employees.
2. Identify the country with the highest number of employees.
3. Determine the most common occupation among the employees.
</instructions>
```


### Step 3: Designing Prompts for Data Parsing Tasks: *Constraints*
Consider including the following constraints in your prompt:


- `Provide me with fully runnable code` – ensures the model creates a ready-to-go solution that you don't need to modify yourself.
- `Only create a simple python script, nothing extra` – some models, especially Claude, tend to overcomplicate the requested code. You might get a solution that is difficult to use due to unnecessary extra functions. This constraint ensures the provided solution only addresses your request.
- `Explain the algorithm in simple words` – if you want to learn more about the code model generated. Usually, models do an excellent job explaining their code.

## Full Prompt Example
Here is a complete example of a well-structured prompt for parsing data with an LLM:

```xml
I have a file called `workers.csv`:

<data_example>
ID, Name, Age, Occupation, Country, Salary, Joining Date, Active
101, Alice Johnson, 30, Engineer, USA, 85000, 2015-06-23, True
102, Bob Smith, 25, Designer, UK, 62000, 2018-09-15, True
103, Charlie Brown, 35, Teacher, Canada, 55000, 2012-01-10, False
</data_example>

<instructions>
Please parse this data and provide the following:
1. Calculate the average salary of all active employees.
2. Identify the country with the highest number of employees.
3. Determine the most common occupation among the employees.
</instructions>

<constraints>
- Provide me with fully runnable code.
- Only create a simple python script, nothing extra.
- Explain the algorithm in simple words.
</constraints>
```

This prompt clearly defines the data sample, instructions, and constraints, making it easy for the LLM to understand and respond accurately.




# 2- Introduction: Debugging with LLMs
In the previous lesson, you learned how to create prompts that help LLMs parse and analyze structured data. Now, let’s focus on a different but equally important skill: debugging code with the help of LLMs.

When your code doesn’t work as expected, it can be frustrating. LLMs can be a powerful tool to help you find and fix bugs, but only if you communicate the problem clearly. In this lesson, I will show you how to share the right information and set clear expectations so the LLM can give you the most helpful answer.

## Recognizing When Code Isn’t Working
Before you can ask for help, you need to know when your code is not working. Usually, you will notice one of two things:

- Your code gives an error message and stops running.
- Your code runs, but the result is not what you expected.

The most common way to spot a problem is through an error message called a traceback. A traceback is a report that shows where the error happened in your code and what kind of error it was.

For example, if you run this code:

```Python
numbers = [10, 20, 30, 40, 50]
total = 0
count = 0
for i in numbers:
    total += i
print(total / count)
```

You might see this error message:

```Plain text
Traceback (most recent call last):
  File "test.py", line 6, in <module>
    print(total / count)
          ~~~~~~^~~~~~~
ZeroDivisionError: division by zero
```

This traceback tells you that the code tried to divide by zero, which is not allowed in Python. The error happened on the line print(total / count).

## What to Share with the LLM
When you ask an LLM for help with debugging, it is important to include both your code and the relevant part of the traceback. If you only share the code, the LLM might not know what went wrong. If you only share the error message, the LLM won’t know what your code looks like.

Let’s take a step-by-step look at a good debugging prompt.

```Plain text
I have a Python script that processes a list of numbers and calculates their average. 
However, the script is throwing an error, and I'm unable to identify the issue. 

Here is the original code:
`script.py`
numbers = [10, 20, 30, 40, 50]
total = 0
count = 0
for i in numbers:
    total += i
print(total / count)

Here is the traceback:

Traceback (most recent call last):
  File "test.py", line 6, in <module>
    print(total / count)
          ~~~~~~^~~~~~~
ZeroDivisionError: division by zero
```

Here, you give the LLM all the information it needs to understand the problem and help you fix it.

## Stating Your Constraints Clearly
LLMs tend to provide more changes than required, especially the Claude models. It is likely to include a lot of excess comments and error-handling code, which may be unnecessary.

To keep your code simple, you might want to add the following constraints:

```Plain text
Provide me with an updated code and a short explanation.

# Constraints

- Don't include any error handling in the code
- Exclude comments in the code
```

By stating your constraints, you help the LLM focus on precisely what you want: a fixed version of your code without extra error handling or comments.

## Sharing the Relevant Part of a Long Traceback
Sometimes, your traceback can be very long, especially if your code uses external libraries or frameworks. In these cases, copying the entire traceback into your prompt is not always helpful. Instead, focus on sharing the most relevant part:

Include the last few lines of the traceback, especially the part that shows the error type and the exact line in your code where the error happened.
If the traceback includes many lines from library code, you can skip those and only include the lines that reference your own files or functions.
For example, if you see a long traceback like this:

```Plain text
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 973, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/home/user/my_script.py", line 12, in run
    result = process_data(data)
  File "/home/user/my_script.py", line 8, in process_data
    return sum(data) / len(data)
ZeroDivisionError: division by zero
```

You can share just the relevant part:

This helps the LLM focus on the part of the traceback that is most useful for debugging your code.