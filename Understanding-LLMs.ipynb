{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c430f0ad",
   "metadata": {},
   "source": [
    "# How LLMs Generate Text: Predicting the Next Token\n",
    "\n",
    "LLMs, like the ones you will use in this course, generate text by predicting one piece at a time. These pieces are called `tokens`. The model looks at the text you have already given (the context) and tries to guess what comes next, one token at a time. This process is repeated until the model finishes its response.\n",
    "\n",
    "Let's break down what a `token` is and how this prediction process works.\n",
    "\n",
    "## What Is a Token?\n",
    "\n",
    "A `token` is a small chunk of text. Depending on the language model, it can be a word, part of a word, or even just a character. LLMs do not see text as whole sentences or paragraphs. Instead, they break everything down into tokens.\n",
    "\n",
    "For example, let's look at the phrase:\n",
    "\n",
    "```\n",
    "A bottle of water\n",
    "```\n",
    "\n",
    "Depending on the model, this might be split into tokens like:\n",
    "\n",
    "- `A`\n",
    "- `bottle`\n",
    "- `of`\n",
    "- `water`\n",
    "\n",
    "Or, in some cases, `bottle` might be split into `bott` and `le` if the model uses smaller pieces. For this lesson, you can think of tokens as words or short word parts.\n",
    "\n",
    "Tokens are important because the model predicts text one token, not one word or sentence at a time.\n",
    "\n",
    "\n",
    "## How LLMs Predict the Next Token\n",
    "\n",
    "Now, let's see how LLMs generate text step by step. The model always looks at the context — the already written tokens — and predicts what comes next.\n",
    "\n",
    "Let's start with a simple prompt:\n",
    "\n",
    "```\n",
    "A bottle of\n",
    "```\n",
    "\n",
    "At this point, the model has three tokens: `A`, `bottle`, and `of`. It now needs to predict the next token. The model looks at the context (`A bottle of`) and tries to guess what is most likely to come next.\n",
    "\n",
    "Common next tokens might be:\n",
    "\n",
    "- `water`\n",
    "- `wine`\n",
    "- `milk`\n",
    "\n",
    "The model chooses the most likely one based on its training. If you continue, the process repeats. For example, if the model predicts `water`, the new context is:\n",
    "\n",
    "```\n",
    "A bottle of water\n",
    "```\n",
    "\n",
    "Now, the model can predict what comes after `water`, such as a period or another word.\n",
    "\n",
    "This process — predicting one token at a time — continues until the model decides the response is complete.\n",
    "\n",
    "## Why Context Matters\n",
    "\n",
    "The `context`, or the tokens that come before, is very important. It changes what the model predicts next. Let's look at two examples to see how context affects the prediction.\n",
    "\n",
    "\n",
    "## How Chat Interfaces Use LLMs as Assistants\n",
    "\n",
    "Modern chat interfaces make it easy to use LLMs as assistants. While LLMs always predict the next token, chat interfaces add instructions or context to guide the model in acting helpfully and following your requests.\n",
    "\n",
    "For example, when you type a question or instruction, the chat interface may add a hidden prompt like \"You are a helpful assistant.\" This tells the LLM to answer your question, follow your instructions, and continue your text.\n",
    "\n",
    "The chat interface also keeps track of the conversation history so the LLM can give more relevant and coherent responses. This setup allows you to interact with LLMs naturally, as if chatting with an assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252bb71d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
