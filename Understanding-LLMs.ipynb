{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c430f0ad",
   "metadata": {},
   "source": [
    "# 1- How LLMs Generate Text: Predicting the Next Token\n",
    "\n",
    "LLMs, like the ones you will use in this course, generate text by predicting one piece at a time. These pieces are called `tokens`. The model looks at the text you have already given (the context) and tries to guess what comes next, one token at a time. This process is repeated until the model finishes its response.\n",
    "\n",
    "Let's break down what a `token` is and how this prediction process works.\n",
    "\n",
    "## What Is a Token?\n",
    "\n",
    "A `token` is a small chunk of text. Depending on the language model, it can be a word, part of a word, or even just a character. LLMs do not see text as whole sentences or paragraphs. Instead, they break everything down into tokens.\n",
    "\n",
    "For example, let's look at the phrase:\n",
    "\n",
    "```bash\n",
    "A bottle of water\n",
    "```\n",
    "\n",
    "Depending on the model, this might be split into tokens like:\n",
    "\n",
    "- `A`\n",
    "- `bottle`\n",
    "- `of`\n",
    "- `water`\n",
    "\n",
    "Or, in some cases, `bottle` might be split into `bott` and `le` if the model uses smaller pieces. For this lesson, you can think of tokens as words or short word parts.\n",
    "\n",
    "Tokens are important because the model predicts text one token, not one word or sentence at a time.\n",
    "\n",
    "\n",
    "## How LLMs Predict the Next Token\n",
    "\n",
    "Now, let's see how LLMs generate text step by step. The model always looks at the context — the already written tokens — and predicts what comes next.\n",
    "\n",
    "Let's start with a simple prompt:\n",
    "\n",
    "```\n",
    "A bottle of\n",
    "```\n",
    "\n",
    "At this point, the model has three tokens: `A`, `bottle`, and `of`. It now needs to predict the next token. The model looks at the context (`A bottle of`) and tries to guess what is most likely to come next.\n",
    "\n",
    "Common next tokens might be:\n",
    "\n",
    "- `water`\n",
    "- `wine`\n",
    "- `milk`\n",
    "\n",
    "The model chooses the most likely one based on its training. If you continue, the process repeats. For example, if the model predicts `water`, the new context is:\n",
    "\n",
    "```\n",
    "A bottle of water\n",
    "```\n",
    "\n",
    "Now, the model can predict what comes after `water`, such as a period or another word.\n",
    "\n",
    "This process — predicting one token at a time — continues until the model decides the response is complete.\n",
    "\n",
    "## Why Context Matters\n",
    "\n",
    "The `context`, or the tokens that come before, is very important. It changes what the model predicts next. Let's look at two examples to see how context affects the prediction.\n",
    "\n",
    "\n",
    "## How Chat Interfaces Use LLMs as Assistants\n",
    "\n",
    "Modern chat interfaces make it easy to use LLMs as assistants. While LLMs always predict the next token, chat interfaces add instructions or context to guide the model in acting helpfully and following your requests.\n",
    "\n",
    "For example, when you type a question or instruction, the chat interface may add a hidden prompt like \"You are a helpful assistant.\" This tells the LLM to answer your question, follow your instructions, and continue your text.\n",
    "\n",
    "The chat interface also keeps track of the conversation history so the LLM can give more relevant and coherent responses. This setup allows you to interact with LLMs naturally, as if chatting with an assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88ebba9",
   "metadata": {},
   "source": [
    "# 2- Understanding Model Versions\n",
    "\n",
    "In this lesson, we will focus on understanding `model versions`. This knowledge will help you make better choices when working with different models and crafting your prompts.\n",
    "\n",
    "\n",
    "### Before we Start\n",
    "You probably already noticed that LLM's answer can differ each time you send a prompt.\n",
    "\n",
    "This is because LLMs are probabilistic—they generate responses by predicting the most likely next word or token, but there's always some randomness involved. This means you can't be sure the model will always return exactly what you want. Sometimes, the answer can be very different with each run of a request, even if you use the same prompt.\n",
    "\n",
    "This is important to remember during the practices: since we can't know exactly what the LLM will return, you may get an unexpected answer. If that happens, you can try rerunning the prompt in another chat to see if you get a better result, or you can refine your prompt to guide the model more clearly.\n",
    "\n",
    "\n",
    "## What Are Model Versions?\n",
    "LLMs are updated and improved over time. Each update is called a `model version`. New versions are released for several reasons:\n",
    "\n",
    "- To include more recent information in the training data.\n",
    "- To fix mistakes or improve how the model understands prompts.\n",
    "- To make the model faster or more accurate.\n",
    "\n",
    "For example, you might see models named `Claude Sonnet 3.7` and `Claude Sonnet 4`. The higher number usually means a newer version, often with more up-to-date information and better performance.\n",
    "\n",
    "\n",
    "### Pay Attention\n",
    "\n",
    "**Note**: Modern language models are typically designed to be cautious when addressing questions outside their knowledge base. If asked about something they have no information on, they often respond honestly with \"I don't know.\" However, it's important to remember that a model's primary function is to generate plausible answers, not necessarily correct ones. As a result, they may sometimes produce inaccurate or misleading information. This is called \"LLM hallucination\".\n",
    "\n",
    "While LLMs can be very helpful assistants, they should not be relied upon as definitive sources of truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98e23d0",
   "metadata": {},
   "source": [
    "# 3- Understanding Reasoning Models: Guiding LLMs to Think Step by Step\n",
    "\n",
    "This lesson is about how you can get even better answers from LLMs — especially for complex or multi-step problems — by guiding them to \"think\" before answering.\n",
    "\n",
    "When you ask a model a simple question, it often gives you a direct answer. However, for more complicated tasks, such as solving a math problem or analyzing a scenario, the model can make mistakes if it tries to answer too quickly. This is where `reasoning techniques` come in. Encouraging the model to break down its thought process can help it arrive at more accurate and logical answers.\n",
    "\n",
    "\n",
    "## Chain of Thought (CoT): The Step-by-Step Approach\n",
    "The `Chain of Thought (CoT)` technique is a way to prompt LLMs to solve problems step by step, just like you might do on paper. Instead of asking for a final answer right away, you guide the model to show its reasoning process.\n",
    "\n",
    "Let's see how this works, starting with a simple math problem.\n",
    "\n",
    "Suppose you ask:\n",
    "\n",
    "```bash\n",
    "Q: What is 197 * 971?\n",
    "```\n",
    "\n",
    "If you ask this, the model will try to predict the next token. It can't actually do the math; it just predicts something plausible. Sometimes, it gets it right, but often, it makes a mistake, especially with large numbers or multi-step problems.\n",
    "\n",
    "To help the model, you can add a phrase like `Think step by step` to your prompt. This tells the model to break down the problem:\n",
    "\n",
    "```bash\n",
    "Q: What is 197 * 971? Think step by step.\n",
    "```\n",
    "But you can be even more helpful by showing the model exactly how to break down the steps. Let's build this up together.\n",
    "\n",
    "First, you can show the model how to multiply using place value:\n",
    "\n",
    "\n",
    "```bash\n",
    "Step 1: Multiply the unit digit of 197 (which is 7) by 971.\n",
    "Step 2: Multiply the tens digit of 197 (which is 9) by 971, and add a zero at the end.\n",
    "Step 3: Multiply the hundreds digit of 197 (which is 1) by 971, and add two zeroes at the end.\n",
    "Step 4: Add all the results together.\n",
    "```\n",
    "\n",
    "Chain of Thought (CoT): Full Example\n",
    "Now, let's put this into a full prompt, step by step:\n",
    "\n",
    "\n",
    "```bash\n",
    "Q: What's 212 * 385?\n",
    "\n",
    "A: Step 1: Multiply 2 (units place of 212) by 385\n",
    "       2 * 385 = 770\n",
    "Step 2: Multiply 1 (tens place of 212) by 385 and add a zero at the end\n",
    "       1 * 385 = 385 => 3850\n",
    "Step 3: Multiply 2 (hundreds place of 212) by 385 and add two zeroes at the end\n",
    "       2 * 385 = 770 => 77000\n",
    "Step 4: Add all the results from steps 1, 2, and 3\n",
    "       770 + 3850 + 77000 = 81620\n",
    "\n",
    "Therefore, 212 * 385 = 81620\n",
    "\n",
    "Q: What's 197 * 971?\n",
    "```\n",
    "\n",
    "In this prompt, we provide the model with a solution example for a different problem and then ask for a solution to an actual problem. This way, we show the model how to \"think\" properly.\n",
    "\n",
    "By guiding the model through each step, you help it avoid mistakes and clarify its reasoning. This is the core idea behind Chain of Thought prompting.\n",
    "\n",
    "You can use this approach for many problems, not just math. For example, you can ask the model to explain its reasoning in logic puzzles, story analysis, or code debugging.\n",
    "\n",
    "\n",
    "## How Reasoning Models Use Chain of Thought\n",
    "\n",
    "Some advanced LLMs are trained to use Chain of Thought reasoning automatically, especially when they see specific cues in your prompt. These are called `reasoning models`. They are designed to handle multi-step problems by breaking them down internally, even if you don't explicitly ask them to.\n",
    "\n",
    "For example, if you prompt a reasoning model with:\n",
    "\n",
    "```bash\n",
    "Q: What's 202*588?\n",
    "```\n",
    "A reasoning model might automatically start breaking down the steps, similar to the example above, and show its work before giving the final answer.\n",
    "\n",
    "However, not all models do this by default. For regular models, you often need to guide them by providing an example or by saying, `Think step by step`. Reasoning models are more likely to follow this process independently, but you can still help them by being clear in your prompt.\n",
    "\n",
    "\n",
    "## When to Use Reasoning Models vs. Regular Models\n",
    "Let's examine the pros and cons of using reasoning models and when to use each type.\n",
    "\n",
    "| Category | Reasoning Models (with CoT)                                                                                                                | Regular Models                                                                            |\n",
    "| -------- | ------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------- |\n",
    "| **Pros** | • Better at multi-step problems (math, logic, analysis)<br>• More transparent reasoning<br>• Less likely to make mistakes on complex tasks | • Faster responses<br>• Good for simple, factual questions<br>• Uses less computing power |\n",
    "| **Cons** | • Slower, longer answers<br>• May over-explain simple questions<br>• Usually more expensive to run                                         | • Can make mistakes on complex tasks<br>• Less transparent reasoning                      |\n",
    "\n",
    "\n",
    "\n",
    "`When to use reasoning models:`\n",
    "\n",
    "- Solving math problems with multiple steps\n",
    "- Logic puzzles or riddles\n",
    "- Explaining something\n",
    "- Brainstorming sessions\n",
    "- Analyzing stories or scenarios\n",
    "- Advanced coding tasks\n",
    "\n",
    "\n",
    "`When to use regular models:`\n",
    "\n",
    "- Quick factual lookups (e.g., \"What is the capital of France?\")\n",
    "- Simple, direct questions\n",
    "- When you want to create something, e.g., generate a story or a code snippet\n",
    "- When you want a short answer\n",
    "- When you want to save money\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa59b2c8",
   "metadata": {},
   "source": [
    "# 4- Understanding Context Windows: Managing Input and Output Sizes\n",
    "\n",
    "A `context window` is the maximum amount of information (measured in tokens) a model can consider simultaneously. This includes your input (the prompt) and the model's output (the response). If you try to give the model more information than fits in its `context window`, some of it will be ignored or cut off.\n",
    "\n",
    "Understanding `context windows` is important because it helps you design prompts that fit within these limits, ensuring the model can \"see\" everything it needs to give you a good answer.\n",
    "\n",
    "\n",
    "## Historical Evolution of Context Limits\n",
    "`Context windows` have changed a lot as LLMs have improved. Early models could only handle short prompts and responses, while newer models can work with much more information at once.\n",
    "\n",
    "Here's a simple table showing how `context window` sizes have grown over time:\n",
    "\n",
    "| Model Name  | Release Year | Context Window Size (tokens) |\n",
    "| ----------- | -----------: | ---------------------------: |\n",
    "| GPT-2       |         2019 |                        1,024 |\n",
    "| GPT-3       |         2020 |                        2,048 |\n",
    "| GPT-3.5     |         2022 |                        4,096 |\n",
    "| GPT-4 (8k)  |         2023 |                        8,192 |\n",
    "| GPT-4 (32k) |         2023 |                       32,768 |\n",
    "| Claude 2    |         2023 |                      100,000 |\n",
    "\n",
    "\n",
    "As you can see, newer models can handle much larger `context windows`. This means you can give them longer prompts or get longer responses, but there is always a limit.\n",
    "\n",
    "\n",
    "## How Context Windows Affect Input and Output\n",
    "Let's look at how the `context window` shapes what you can do with an LLM.\n",
    "\n",
    "The `context window` is shared between your input and the model's output. For example, if a model has a 4,096-token `context window` and your prompt is 2,000 tokens, it can only generate up to 2,096 tokens in its response. If your prompt is too long, the model's response will be shorter or cut off part of your input.\n",
    "\n",
    "\n",
    "## Strategies: Make Inputs Shorter and More Relevant\n",
    "When you hit the `context window` limit, you have several strategies to get the desired results. Let's go through them step by step, with examples.\n",
    "\n",
    "Instead of pasting everything, focus on the most important parts.\n",
    "\n",
    "Suppose you have a long email thread but only need a summary of the last conversation.\n",
    "\n",
    "Prompt:\n",
    "\n",
    "```bash\n",
    "Summarize the last two emails in this thread:\n",
    "[Paste only the previous two emails]\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Including only the relevant emails saves space and ensures the model focuses on what matters.\n",
    "\n",
    "\n",
    "\n",
    "## Strategies: Request Shorter or Partial Outputs\n",
    "If you can't fit everything, ask the model for a recommendation or a plan, not the full result.\n",
    "\n",
    "Suppose you want to improve a lengthy document, but it's too big for the `context window`.\n",
    "\n",
    "Prompt:\n",
    "\n",
    "```bash\n",
    "Read the introduction of this document and suggest three ways to improve it:\n",
    "[Paste only the introduction]\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Instead of asking for a complete rewrite, you ask for suggestions. This fits within the `context window` and still gives you helpful feedback.\n",
    "\n",
    "\n",
    "## Strategies: Break into Parts\n",
    "Break the task into smaller steps. Suppose you have a book chapter to summarize.\n",
    "\n",
    "Prompt 1:\n",
    "\n",
    "```bash\n",
    "Summarize pages 1-3 of this chapter:\n",
    "[Paste text from pages 1-3]\n",
    "```\n",
    "\n",
    "Prompt 2:\n",
    "\n",
    "```bash\n",
    "Summarize pages 4-6 of this chapter:\n",
    "[Paste text from pages 4-6]\n",
    "```\n",
    "\n",
    "Prompt 3:\n",
    "\n",
    "```bash\n",
    "Combine these summaries into a single summary:\n",
    "[Paste the two summaries]\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "\n",
    "By summarizing in smaller chunks and then combining the results, you work around the `context window` limit.\n",
    "\n",
    "Of course, there are other strategies, including `advanced iterative approaches`. We will explore these approaches in the following courses of this path.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252bb71d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
